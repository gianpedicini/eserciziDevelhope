{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "take the dag we built in class and add a last task which removes the dataset_raw.txt from the source folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime,timedelta\n",
    "\n",
    "\n",
    "default_dag_args={\n",
    "    \"start_date\": datetime(2022,1,1),\n",
    "    \"email_on_failure\": False,\n",
    "    \"email_on_retry\": False,\n",
    "    \"retries\":1,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "    \"project_id\": 1\n",
    "}\n",
    "\n",
    "#let's define our DAG\n",
    "\n",
    "with DAG(\"First_DAG\",schedule_interval=None, default_args= default_dag_args) as dag:\n",
    "\n",
    "    #here at this level we define out tasks\n",
    "    t0= BashOperator(task_id=\"bash_task\", bash_command=\"echo 'command executed from Bash Operator'\")\n",
    "    t1= BashOperator(task_id=\"bash_task_move_data\", bash_command=\"cp /Users/gian/Desktop/Develhope/DATA_CENTER/DATA_LAKE/dataset_raw.txt /Users/gian/Desktop/Develhope/DATA_CENTER/CLEAN_DATA \")\n",
    "    t2= BashOperator(task_id=\"bash_task_remove_file\", bash_command=\"rm /Users/gian/Desktop/Develhope/DATA_CENTER/DATA_LAKE/dataset_raw.txt\")\n",
    "\n",
    "    #in the end of your DAG definition, we want to write the dependencies of the tasks\n",
    "\n",
    "    t0 >> t1 >> t2"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
